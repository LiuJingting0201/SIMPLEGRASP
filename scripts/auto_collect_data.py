#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–å¯ä¾›æ€§è®­ç»ƒæ•°æ®æ”¶é›†å™¨
Automatic Affordance Training Data Collector

åŸºäºå·²éªŒè¯çš„å·¥ä½œç³»ç»Ÿï¼Œè‡ªåŠ¨æ”¶é›†è®­ç»ƒæ•°æ®
"""

import numpy as np
import cv2
import os
import json
import time
import argparse
import pybullet as p
from PIL import Image
from pathlib import Path
import sys

# å¯¼å…¥å·¥ä½œçš„æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from src.environment_setup import setup_environment
from src.perception import set_topdown_camera, get_rgb_depth_segmentation, pixel_to_world
from src.afford_data_gen import sample_grasp_candidates, fast_grasp_test, reset_robot_home

class AutoAffordanceCollector:
    """è‡ªåŠ¨åŒ–å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, data_dir="data/affordance_v5", num_angles=8, train_split=0.8):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•å­ç›®å½•
        self.train_dir = self.data_dir / "train"
        self.test_dir = self.data_dir / "test"
        self.train_dir.mkdir(exist_ok=True)
        self.test_dir.mkdir(exist_ok=True)
        
        self.num_angles = num_angles
        self.train_split = train_split  # è®­ç»ƒé›†æ¯”ä¾‹ (0.8 = 80%)
        
        # æŠ“å–è§’åº¦è®¾ç½®
        self.grasp_angles = np.linspace(0, 2*np.pi, num_angles, endpoint=False)
        
        print(f"ğŸ¯ è‡ªåŠ¨åŒ–å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir.absolute()}")
        print(f"   è®­ç»ƒç›®å½•: {self.train_dir}")
        print(f"   æµ‹è¯•ç›®å½•: {self.test_dir}")
        print(f"   è®­ç»ƒé›†æ¯”ä¾‹: {train_split:.1%}")
        print(f"   æŠ“å–è§’åº¦æ•°: {num_angles}")
    
    def collect_single_attempt_as_scene(self, scene_idx, robot_id, object_ids, target_dir):
        """æ”¶é›†å•æ¬¡æŠ“å–å°è¯•ä½œä¸ºç‹¬ç«‹åœºæ™¯ï¼Œæ¯æ¬¡æŠ“å–å‰æ¢å¤ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€"""
        print(f"============================================================")
        
        # 1. è®°å½•åˆå§‹æœºå™¨äººå’Œç‰©ä½“çŠ¶æ€
        home_joints = [0, -0.785, 0, -2.356, 0, 1.571, 0.785]
        initial_robot_joints = [p.getJointState(robot_id, i)[0] for i in range(7)]
        initial_gripper = [p.getJointState(robot_id, 9)[0], p.getJointState(robot_id, 10)[0]]
        object_states = {}
        for obj_id in object_ids:
            pos, orn = p.getBasePositionAndOrientation(obj_id)
            object_states[obj_id] = (pos, orn)
        
        # 2. ç¡®ä¿æœºå™¨äººåœ¨åˆå§‹ä½ç½®
        print("   ğŸ  é‡ç½®æœºå™¨äºº...")
        for i in range(7):
            p.setJointMotorControl2(
                robot_id, i, p.POSITION_CONTROL,
                targetPosition=home_joints[i], 
                force=500, 
                maxVelocity=2.0
            )
        for _ in range(200):
            p.stepSimulation()
            all_in_position = True
            for i in range(7):
                current = p.getJointState(robot_id, i)[0]
                if abs(current - home_joints[i]) > 0.05:
                    all_in_position = False
                    break
            if all_in_position:
                break
        # å¼ºåˆ¶æ‰“å¼€å¤¹çˆª
        p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, targetPosition=0.02, force=300)
        p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, targetPosition=0.02, force=300)
        for _ in range(40):
            p.stepSimulation()
        # éªŒè¯æœºå™¨äººä½ç½®
        current_pos = p.getLinkState(robot_id, 8)[0]
        print(f"   ğŸ“ æœºå™¨äººæœ«ç«¯ä½ç½®: [{current_pos[0]:.3f}, {current_pos[1]:.3f}, {current_pos[2]:.3f}]")
        # æ‹æ‘„ç…§ç‰‡
        print("   ğŸ“· æ‹æ‘„åœºæ™¯ç…§ç‰‡...")
        width, height, view_matrix, proj_matrix = set_topdown_camera()
        rgb_image, depth_image, seg_mask = get_rgb_depth_segmentation(width, height, view_matrix, proj_matrix)
        print(f"   ğŸ“· ç›¸æœºæ•°æ®: RGB {rgb_image.shape}, æ·±åº¦ {depth_image.shape}")
        # é‡‡æ ·æŠ“å–å€™é€‰ç‚¹
        candidates = sample_grasp_candidates(
            depth=depth_image,
            num_angles=self.num_angles,
            visualize=False,
            rgb=rgb_image,
            view_matrix=view_matrix,
            proj_matrix=proj_matrix,
            seg_mask=seg_mask,
            object_ids=object_ids
        )
        
        print(f"   ğŸ” å€™é€‰ç‚¹é‡‡æ ·ç»“æœ: {len(candidates)} ä¸ªå€™é€‰ç‚¹")
        
        if not candidates:
            print("   âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æŠ“å–å€™é€‰ç‚¹")
            return False
        
        print(f"   ğŸ“ é‡‡æ ·äº† {len(candidates)} ä¸ªå€™é€‰ç‚¹ - æµ‹è¯•å‰ {min(20, len(candidates))} ä¸ª")
        
        # æµ‹è¯•å¤šä¸ªå€™é€‰ç‚¹ï¼ˆæ¯ä¸ªåœºæ™¯æœ€å¤š20ä¸ªï¼Œæä¾›æ›´å¯†é›†çš„è®­ç»ƒæ ‡ç­¾ï¼‰
        test_count = min(20, len(candidates))
        successful_grasps = []
        failed_grasps = []
        
        for i, candidate in enumerate(candidates[:test_count]):
            if len(candidate) == 4:
                u, v, theta_idx, theta = candidate
            else:
                u, v, theta_idx = candidate
                theta = self.grasp_angles[theta_idx]
            world_pos = pixel_to_world(u, v, depth_image[v, u], view_matrix, proj_matrix)
            print(f"   ğŸ¯ æµ‹è¯• {i+1}/{test_count}: åƒç´ ({u},{v}), è§’åº¦{np.degrees(theta):.1f}Â°")
            # æ¯æ¬¡æŠ“å–å‰æ¢å¤æœºå™¨äººå’Œç‰©ä½“åˆ°åˆå§‹çŠ¶æ€
            if i > 0:
                print(f"      ğŸ  æ¢å¤æœºå™¨äººå’Œç‰©ä½“åˆ°åˆå§‹çŠ¶æ€...")
                # æ¢å¤æœºå™¨äººå…³èŠ‚
                for j in range(7):
                    p.resetJointState(robot_id, j, home_joints[j])
                # æ¢å¤å¤¹çˆª
                p.resetJointState(robot_id, 9, 0.02)
                p.resetJointState(robot_id, 10, 0.02)
                # æ¢å¤ç‰©ä½“
                for obj_id in object_ids:
                    pos, orn = object_states[obj_id]
                    p.resetBasePositionAndOrientation(obj_id, pos, orn)
                # è®©ç‰©ç†å¼•æ“ç¨³å®š
                for _ in range(10):
                    p.stepSimulation()
            # æ‰§è¡ŒæŠ“å–æµ‹è¯•
            success = fast_grasp_test(robot_id, world_pos, theta, object_ids, visualize=False)
            if success:
                print(f"      âœ… æˆåŠŸ!")
                successful_grasps.append({'pixel': (u, v), 'angle': theta, 'world_pos': world_pos})
            else:
                print(f"      âŒ å¤±è´¥")
                failed_grasps.append({'pixel': (u, v), 'angle': theta, 'world_pos': world_pos})
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = len(successful_grasps) / test_count if test_count > 0 else 0
        print(f"   ğŸ“Š åœºæ™¯æˆåŠŸç‡: {len(successful_grasps)}/{test_count} ({success_rate:.1%})")
        
        # ç”Ÿæˆå¯ä¾›æ€§æ ‡ç­¾ï¼ˆæ ‡è®°æ‰€æœ‰æµ‹è¯•çš„ç‚¹ï¼‰
        affordance_map = np.zeros((224, 224), dtype=np.float32)
        angle_map = np.zeros((224, 224), dtype=np.float32) 
        
        # æ ‡è®°æˆåŠŸçš„æŠ“å–ç‚¹
        for grasp in successful_grasps:
            u, v = grasp['pixel']
            theta = grasp['angle']
            affordance_map[v, u] = 1.0  # æˆåŠŸç‚¹æ ‡è®°ä¸º1
            angle_map[v, u] = theta
        
        # å¤±è´¥çš„ç‚¹ä¿æŒä¸º0ï¼ˆå·²ç»åˆå§‹åŒ–ä¸º0ï¼‰
        
        # ä¿å­˜æ•°æ®
        data_dir = target_dir
        os.makedirs(data_dir, exist_ok=True)
        
        scene_prefix = f"scene_{scene_idx:04d}"
        
        # ä¿å­˜å›¾åƒ
        rgb_path = os.path.join(data_dir, f"{scene_prefix}_rgb.png")
        Image.fromarray(rgb_image).save(rgb_path)
        
        # ä¿å­˜æ·±åº¦å›¾
        depth_path = os.path.join(data_dir, f"{scene_prefix}_depth.npy")
        np.save(depth_path, depth_image)
        
        # ä¿å­˜å¯ä¾›æ€§å›¾
        affordance_path = os.path.join(data_dir, f"{scene_prefix}_affordance.npy")
        np.save(affordance_path, affordance_map)
        
        # ä¿å­˜è§’åº¦å›¾
        angle_path = os.path.join(data_dir, f"{scene_prefix}_angles.npy")
        np.save(angle_path, angle_map)
        
        # ä¿å­˜å…ƒæ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰æŠ“å–å°è¯•çš„è¯¦ç»†ä¿¡æ¯ï¼‰
        meta_data = {
            "scene_id": scene_idx,
            "total_attempts": test_count,
            "successful_grasps": len(successful_grasps),
            "failed_grasps": len(failed_grasps),
            "success_rate": success_rate,
            "image_size": [int(rgb_image.shape[1]), int(rgb_image.shape[0])],
            "num_objects": len(object_ids),
            "object_ids": [int(oid) for oid in object_ids],
            "grasp_details": {
                "successful": [
                    {
                        "pixel": [int(g['pixel'][0]), int(g['pixel'][1])],
                        "world_pos": [float(g['world_pos'][0]), float(g['world_pos'][1]), float(g['world_pos'][2])],
                        "angle_degrees": float(np.degrees(g['angle']))
                    } for g in successful_grasps
                ],
                "failed": [
                    {
                        "pixel": [int(g['pixel'][0]), int(g['pixel'][1])],
                        "world_pos": [float(g['world_pos'][0]), float(g['world_pos'][1]), float(g['world_pos'][2])],
                        "angle_degrees": float(np.degrees(g['angle']))
                    } for g in failed_grasps
                ]
            }
        }
        
        meta_path = os.path.join(data_dir, f"{scene_prefix}_meta.json")
        with open(meta_path, 'w') as f:
            json.dump(meta_data, f, indent=2)
        
        print(f"      ğŸ’¾ æ•°æ®å·²ä¿å­˜: {scene_prefix}_*")
        
        # è¿”å›æ˜¯å¦æœ‰ä»»ä½•æˆåŠŸçš„æŠ“å–
        return len(successful_grasps) > 0
        """æ”¶é›†å•ä¸ªåœºæ™¯æ•°æ® - ä¿®å¤ä¸ºæ­£ç¡®çš„åœºæ™¯å®šä¹‰
        
        æ­£ç¡®çš„åœºæ™¯å®šä¹‰ï¼š
        1. ä¸€ä¸ªåœºæ™¯ = ä¸€å¼ ç…§ç‰‡ + å¤šæ¬¡æŠ“å–å°è¯•
        2. æœºå™¨äººåœ¨æ¯æ¬¡æŠ“å–å‰å›åˆ°åˆå§‹ä½ç½®
        3. ç…§ç‰‡åªæ‹ä¸€æ¬¡ï¼ˆæœºå™¨äººåœ¨åˆå§‹ä½ç½®æ—¶ï¼‰
        4. å¤šä¸ªå€™é€‰ç‚¹åœ¨åŒä¸€åœºæ™¯ä¸­æµ‹è¯•
        """
        try:
            # 1. è®¾ç½®ç¯å¢ƒ
            robot_id, object_ids = setup_environment(num_objects=num_objects)
            if not object_ids:
                return False
            
            # 2. ç­‰å¾…ç‰©ä½“ç¨³å®š
            for _ in range(120):
                p.stepSimulation()
            
                        # 3. ç¡®ä¿æœºå™¨äººåœ¨åˆå§‹ä½ç½® - ä½¿ç”¨ä½ç½®æ§åˆ¶
            print("   ğŸ  é‡ç½®æœºå™¨äºº...")
            
            home_joints = [0, -0.785, 0, -2.356, 0, 1.571, 0.785]
            
            # ä½¿ç”¨ä½ç½®æ§åˆ¶è€Œä¸æ˜¯ç›´æ¥è®¾ç½®å…³èŠ‚çŠ¶æ€ï¼Œæ›´å¹³æ»‘
            for i in range(7):
                p.setJointMotorControl2(
                    robot_id, i, p.POSITION_CONTROL,
                    targetPosition=home_joints[i], 
                    force=500, 
                    maxVelocity=2.0
                )
            
            # ç­‰å¾…åˆ°ä½
            for _ in range(200):  # å¢åŠ ç­‰å¾…æ—¶é—´
                p.stepSimulation()
                
                # æ£€æŸ¥æ˜¯å¦åˆ°ä½
                all_in_position = True
                for i in range(7):
                    current = p.getJointState(robot_id, i)[0]
                    if abs(current - home_joints[i]) > 0.05:  # å®¹å·®3åº¦
                        all_in_position = False
                        break
                
                if all_in_position:
                    break
            
            # å¼ºåˆ¶æ‰“å¼€å¤¹çˆª
            p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, targetPosition=0.02, force=300)
            p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, targetPosition=0.02, force=300)
            for _ in range(40):
                p.stepSimulation()
            
            # éªŒè¯æœºå™¨äººæ˜¯å¦çœŸçš„åˆ°äº†åˆå§‹ä½ç½®
            ee_link = 11
            current_pos = p.getLinkState(robot_id, ee_link)[0]
            print(f"   ğŸ“ æœºå™¨äººæœ«ç«¯ä½ç½®: [{current_pos[0]:.3f}, {current_pos[1]:.3f}, {current_pos[2]:.3f}]")
            
            # 4. æ‹æ‘„ç…§ç‰‡ï¼ˆæœºå™¨äººåœ¨åˆå§‹ä½ç½®æ—¶ï¼‰
            print("   ğŸ“· æ‹æ‘„åœºæ™¯ç…§ç‰‡...")
            width, height, view_matrix, proj_matrix = set_topdown_camera()
            rgb_image, depth_image, seg_mask = get_rgb_depth_segmentation(width, height, view_matrix, proj_matrix)
            print(f"   ğŸ“· ç›¸æœºæ•°æ®: RGB {rgb_image.shape}, æ·±åº¦ {depth_image.shape}")
            
            # 5. é‡‡æ ·æŠ“å–å€™é€‰ç‚¹
            candidates = sample_grasp_candidates(
                depth=depth_image,
                num_angles=self.num_angles,
                visualize=False,
                rgb=rgb_image,
                view_matrix=view_matrix,
                proj_matrix=proj_matrix,
                seg_mask=seg_mask,
                object_ids=object_ids
            )
            
            print(f"   ğŸ” å€™é€‰ç‚¹é‡‡æ ·ç»“æœ: {len(candidates)} ä¸ªå€™é€‰ç‚¹")
            
            if not candidates:
                print("   âŒ æ²¡æœ‰å€™é€‰ç‚¹")
                return False
            
            # 6. æµ‹è¯•å¤šä¸ªå€™é€‰ç‚¹ - è¿™æ˜¯å…³é”®ä¿®å¤
            results = []
            success_count = 0
            test_count = min(max_attempts_per_scene, len(candidates))  # æœ€å¤šæµ‹è¯•æŒ‡å®šæ•°é‡
            
            print(f"   ğŸ¯ æµ‹è¯• {test_count} ä¸ªå€™é€‰ç‚¹...")
            
            for i, candidate in enumerate(candidates[:test_count]):
                if len(candidate) == 4:
                    u, v, theta_idx, theta = candidate
                else:
                    u, v, theta_idx = candidate
                    theta = self.grasp_angles[theta_idx]
                
                world_pos = pixel_to_world(u, v, depth_image[v, u], view_matrix, proj_matrix)
                
                print(f"      ğŸ¯ æµ‹è¯• {i+1}/{test_count}: åƒç´ ({u},{v}), è§’åº¦{np.degrees(theta):.1f}Â°")
                
                # ğŸ”‘ å…³é”®ï¼šæ¯æ¬¡æŠ“å–å‰ç¡®ä¿æœºå™¨äººåœ¨åˆå§‹ä½ç½®
                if i > 0:  # ç¬¬ä¸€æ¬¡ä¸éœ€è¦é‡ç½®ï¼Œå·²ç»åœ¨åˆå§‹ä½ç½®
                    print(f"      ğŸ  é‡ç½®æœºå™¨äººä½ç½®...")
                    end_pos_before = p.getLinkState(robot_id, 8)[0]
                    print(f"      ğŸ“ å½“å‰æœ«ç«¯ä½ç½®: [{end_pos_before[0]:.3f}, {end_pos_before[1]:.3f}, {end_pos_before[2]:.3f}]")
                    
                    # ä½¿ç”¨ä½ç½®æ§åˆ¶é‡ç½®
                    home = [0, -0.785, 0, -2.356, 0, 1.571, 0.785]
                    for j in range(7):
                        p.setJointMotorControl2(
                            robot_id, j, p.POSITION_CONTROL,
                            targetPosition=home[j], 
                            force=500, 
                            maxVelocity=2.0
                        )
                    
                    # ç­‰å¾…åˆ°ä½
                    for _ in range(150):
                        p.stepSimulation()
                        
                        # æ£€æŸ¥æ˜¯å¦åˆ°ä½
                        all_in_position = True
                        for j in range(7):
                            current = p.getJointState(robot_id, j)[0]
                            if abs(current - home[j]) > 0.05:
                                all_in_position = False
                                break
                        
                        if all_in_position:
                            break
                    
                    # å¼ºåˆ¶æ‰“å¼€å¤¹çˆª
                    p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, targetPosition=0.02, force=300)
                    p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, targetPosition=0.02, force=300)
                    
                    # ç­‰å¾…æœºå™¨äººå®Œå…¨ç¨³å®š
                    for _ in range(150):
                        p.stepSimulation()
                        time.sleep(1./240.)
                    
                    # éªŒè¯æœºå™¨äººæ˜¯å¦çœŸçš„åˆ°äº†åˆå§‹ä½ç½®
                    ee_link = 11
                    current_pos = p.getLinkState(robot_id, ee_link)[0]
                    print(f"      ğŸ“ å½“å‰æœ«ç«¯ä½ç½®: [{current_pos[0]:.3f}, {current_pos[1]:.3f}, {current_pos[2]:.3f}]")
                    
                    # è°ƒè¯•: æ‰“å°ç‰©ä½“ä½ç½®
                    print(f"      ğŸ” é‡ç½®åç‰©ä½“ä½ç½®:")
                    for j, obj_id in enumerate(object_ids):
                        pos, orn = p.getBasePositionAndOrientation(obj_id)
                        print(f"         ç‰©ä½“ {obj_id}: ä½ç½® [{pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f}]")
                
                # æµ‹è¯•æŠ“å–
                try:
                    success = fast_grasp_test(
                        robot_id=robot_id,
                        world_pos=world_pos,
                        grasp_angle=theta,
                        object_ids=object_ids,
                        visualize=False,
                        debug_mode=False
                    )
                    
                    if success:
                        success_count += 1
                        print(f"      âœ… æˆåŠŸ!")
                    else:
                        print(f"      âŒ å¤±è´¥")
                    
                    # è®°å½•ç»“æœ
                    results.append({
                        'pixel': [int(u), int(v)],
                        'world_pos': [float(world_pos[0]), float(world_pos[1]), float(world_pos[2])],
                        'angle': float(theta),
                        'angle_idx': int(theta_idx),
                        'success': success
                    })
                    
                except Exception as e:
                    print(f"      âŒ é”™è¯¯: {e}")
                    results.append({
                        'pixel': [int(u), int(v)],
                        'world_pos': [0, 0, 0],
                        'angle': float(theta),
                        'angle_idx': int(theta_idx),
                        'success': False
                    })
            
            # 7. è®¡ç®—æˆåŠŸç‡å¹¶ä¿å­˜æ•°æ®
            success_rate = (success_count / test_count) * 100 if test_count > 0 else 0
            print(f"   ğŸ“Š æˆåŠŸç‡: {success_count}/{test_count} ({success_rate:.1f}%)")
            
            # 8. ç”Ÿæˆå¹¶ä¿å­˜åœºæ™¯æ•°æ®
            affordance_map = self.create_affordance_map(rgb_image.shape[:2], results)
            angle_map = self.create_angle_map(rgb_image.shape[:2], results)
            
            self.save_scene_data(scene_id, rgb_image, depth_image, affordance_map, angle_map, results)
            print(f"      ğŸ’¾ æ•°æ®å·²ä¿å­˜: scene_{scene_id:04d}_*")
            print(f"   âœ… åœºæ™¯ {scene_id} å®Œæˆ (æˆåŠŸç‡: {success_rate:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f"   âŒ åœºæ™¯é”™è¯¯: {e}")
            return False
    
    def is_position_reachable(self, world_pos):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨æœºå™¨äººå·¥ä½œç©ºé—´å†…"""
        x, y, z = world_pos
        
        # åŸºæœ¬å·¥ä½œç©ºé—´é™åˆ¶
        distance = np.sqrt(x**2 + y**2)
        
        # Franka Pandaçš„å·¥ä½œç©ºé—´çº¦æŸ
        if distance < 0.3 or distance > 0.85:  # è·ç¦»é™åˆ¶
            return False
        if abs(y) > 0.4:  # Yè½´é™åˆ¶
            return False
        if z < 0.58 or z > 0.8:  # Zè½´é«˜åº¦é™åˆ¶
            return False
        if x < 0.2 or x > 0.9:  # Xè½´å‰åé™åˆ¶
            return False
            
        return True
    
    def is_position_reachable(self, world_pos):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨æœºå™¨äººå·¥ä½œç©ºé—´å†…"""
        x, y, z = world_pos
        
        # åŸºæœ¬å·¥ä½œç©ºé—´é™åˆ¶
        distance = np.sqrt(x**2 + y**2)
        
        # Franka Pandaçš„å·¥ä½œç©ºé—´çº¦æŸ
        if distance < 0.3 or distance > 0.85:  # è·ç¦»é™åˆ¶
            return False
        if abs(y) > 0.4:  # Yè½´é™åˆ¶
            return False
        if z < 0.58 or z > 0.8:  # Zè½´é«˜åº¦é™åˆ¶
            return False
        if x < 0.2 or x > 0.9:  # Xè½´å‰åé™åˆ¶
            return False
            
        return True
    
    def generate_affordance_map(self, results, image_shape):
        """ç”Ÿæˆå¯ä¾›æ€§çƒ­åŠ›å›¾"""
        affordance_map = np.zeros(image_shape, dtype=np.float32)
        
        for result in results:
            if result['world_pos'][0] != 0:  # æœ‰æ•ˆçš„ä¸–ç•Œåæ ‡
                u, v = result['pixel']
                if 0 <= v < image_shape[0] and 0 <= u < image_shape[1]:
                    # æˆåŠŸä¸º1.0ï¼Œå¤±è´¥ä¸º0.0
                    affordance_map[v, u] = 1.0 if result['success'] else 0.0
        
        # è½»å¾®é«˜æ–¯æ¨¡ç³Šæ¥å¹³æ»‘çƒ­åŠ›å›¾
        affordance_map = cv2.GaussianBlur(affordance_map, (5, 5), 1.0)
        return affordance_map
    
    def generate_angle_map(self, results, image_shape):
        """ç”Ÿæˆæœ€ä½³æŠ“å–è§’åº¦åœ°å›¾"""
        angle_map = np.zeros(image_shape, dtype=np.float32)
        
        for result in results:
            if result['success'] and result['world_pos'][0] != 0:
                u, v = result['pixel']
                if 0 <= v < image_shape[0] and 0 <= u < image_shape[1]:
                    # å½’ä¸€åŒ–è§’åº¦åˆ° [0, 1]
                    normalized_angle = result['angle'] / (2 * np.pi)
                    angle_map[v, u] = normalized_angle
        
        return angle_map
    
    def create_affordance_map(self, image_shape, results):
        """åˆ›å»ºå¯ä¾›æ€§åœ°å›¾"""
        height, width = image_shape
        affordance_map = np.zeros((height, width), dtype=np.float32)
        
        for result in results:
            if 'pixel' in result and len(result['pixel']) == 2:
                u, v = result['pixel']
                if 0 <= v < height and 0 <= u < width:
                    affordance_map[v, u] = 1.0 if result['success'] else 0.0
        
        return affordance_map
    
    def create_angle_map(self, image_shape, results):
        """åˆ›å»ºè§’åº¦åœ°å›¾"""
        height, width = image_shape
        angle_map = np.zeros((height, width), dtype=np.float32)
        
        for result in results:
            if result['success'] and result['world_pos'][0] != 0:
                u, v = result['pixel']
                if 0 <= v < height and 0 <= u < width:
                    angle_map[v, u] = result['angle']
        
        return angle_map
    
    def save_scene_data(self, scene_id, rgb_image, depth_image, affordance_map, angle_map, results):
        """ä¿å­˜åœºæ™¯æ•°æ®"""
        scene_prefix = f"scene_{scene_id:04d}"
        
        # ä¿å­˜å›¾åƒ
        rgb_path = self.data_dir / f"{scene_prefix}_rgb.png"
        cv2.imwrite(str(rgb_path), cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
        
        # ä¿å­˜æ·±åº¦å›¾
        depth_path = self.data_dir / f"{scene_prefix}_depth.npy"
        np.save(depth_path, depth_image)
        
        # ä¿å­˜å¯ä¾›æ€§åœ°å›¾
        affordance_path = self.data_dir / f"{scene_prefix}_affordance.npy"
        np.save(affordance_path, affordance_map)
        
        # ä¿å­˜è§’åº¦åœ°å›¾
        angle_path = self.data_dir / f"{scene_prefix}_angles.npy"
        np.save(angle_path, angle_map)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'scene_id': scene_id,
            'image_shape': rgb_image.shape[:2],
            'num_candidates': len(results),
            'num_successful': sum(1 for r in results if r['success']),
            'success_rate': sum(1 for r in results if r['success']) / len(results) if results else 0,
            'candidates': results
        }
        
        meta_path = self.data_dir / f"{scene_prefix}_meta.json"
        with open(meta_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"      ğŸ’¾ æ•°æ®å·²ä¿å­˜: {scene_prefix}_*")
    
    def collect_dataset(self, num_scenes, num_objects_range=(2, 4), max_attempts_per_scene=25):
        """æ”¶é›†å®Œæ•´æ•°æ®é›†"""
        print(f"\nğŸš€ å¼€å§‹æ”¶é›†æ•°æ®é›†")
        print(f"   åœºæ™¯æ•°é‡: {num_scenes}")
        print(f"   ç‰©ä½“æ•°é‡èŒƒå›´: {num_objects_range[0]}-{num_objects_range[1]}")
        print(f"   æ¯åœºæ™¯æœ€å¤§å°è¯•æ•°: {max_attempts_per_scene}")
        print("=" * 60)
        
        successful_scenes = 0
        total_grasps = 0
        total_successful_grasps = 0
        
        for scene_id in range(num_scenes):
            # æ¯5ä¸ªåœºæ™¯åˆ›å»ºä¸€ä¸ªæ–°çš„ç¯å¢ƒé…ç½®
            if scene_id % 5 == 0:
                # æ¸…ç†ä¹‹å‰çš„ç¯å¢ƒ
                if scene_id > 0:
                    for obj_id in object_ids:
                        try:
                            p.removeBody(obj_id)
                        except:
                            pass
                
                # éšæœºé€‰æ‹©ç‰©ä½“æ•°é‡å¹¶åˆ›å»ºæ–°ç¯å¢ƒ
                num_objects = np.random.randint(num_objects_range[0], num_objects_range[1] + 1)
                
                # âœ¨ ç¡®ä¿éšæœºæ€§ï¼šä¸ºæ¯ä¸ªæ–°ç¯å¢ƒè®¾ç½®ä¸åŒçš„éšæœºç§å­
                np.random.seed(scene_id + int(time.time()) % 1000)
                
                print(f"ğŸ—ï¸  åˆ›å»ºæ–°ç¯å¢ƒé…ç½® #{(scene_id // 5) + 1}: {num_objects} ä¸ªç‰©ä½“")
                robot_id, object_ids = setup_environment(num_objects=num_objects)
                if not object_ids:
                    print(f"   âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œè·³è¿‡åœºæ™¯ {scene_id}")
                    continue
                
                # è°ƒè¯•: æ‰“å°ç‰©ä½“ä½ç½®å’Œæœºå™¨äººå§¿æ€
                print(f"ğŸ” åœºæ™¯ {scene_id} ç¯å¢ƒè®¾ç½®è°ƒè¯•:")
                robot_pos = p.getLinkState(robot_id, 8)[0]
                print(f"  ğŸ“ æœºå™¨äººæœ«ç«¯ä½ç½®: {robot_pos}")
                print(f"  ğŸ“ ç‰©ä½“ä½ç½®:")
                for i, obj_id in enumerate(object_ids):
                    pos, orn = p.getBasePositionAndOrientation(obj_id)
                    print(f"    ç‰©ä½“ {i}: ä½ç½®={pos}, æœå‘={orn}")
            
            # æ”¶é›†å•æ¬¡æŠ“å–å°è¯•ä½œä¸ºç‹¬ç«‹åœºæ™¯
            print(f"   ğŸ” åœºæ™¯ {scene_id} å¼€å§‹ - ç‰©ä½“IDs: {object_ids}")
            
            # éšæœºå†³å®šè¯¥åœºæ™¯å±äºè®­ç»ƒé›†è¿˜æ˜¯æµ‹è¯•é›†
            is_train = np.random.random() < self.train_split
            target_dir = self.train_dir if is_train else self.test_dir
            split_name = "è®­ç»ƒé›†" if is_train else "æµ‹è¯•é›†"
            print(f"   ğŸ“Š åœºæ™¯åˆ†é…: {split_name} ({target_dir.name})")
            
            success = self.collect_single_attempt_as_scene(scene_id, robot_id, object_ids, target_dir)
            
            # è¯»å–åœºæ™¯ç»Ÿè®¡ä¿¡æ¯
            import json
            meta_file = os.path.join(target_dir, f"scene_{scene_id:04d}_meta.json")
            if os.path.exists(meta_file):
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                    scene_attempts = meta.get('total_attempts', 0)
                    scene_successes = meta.get('successful_grasps', 0)
                    total_grasps += scene_attempts
                    total_successful_grasps += scene_successes
            
            # åœºæ™¯é—´çŸ­æš‚åœé¡¿ï¼Œè®©ç‰©ç†å¼•æ“ç¨³å®š
            if scene_id < num_scenes - 1:
                for _ in range(10):
                    p.stepSimulation()
                    time.sleep(1./240.)
            
            if success:
                successful_scenes += 1
                print(f"   âœ… åœºæ™¯ {scene_id} å®Œæˆ (æœ‰å¯ç”¨æ•°æ®)")
            else:
                print(f"   âŒ åœºæ™¯ {scene_id} å®Œæˆ (æ— å¯ç”¨æ•°æ®)")
        
        # æ€»ç»“
        grasp_success_rate = (total_successful_grasps / total_grasps) if total_grasps > 0 else 0
        print("=" * 60)
        print(f"ğŸ‰ æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   æœ‰æ•ˆåœºæ™¯: {successful_scenes}/{num_scenes}")
        print(f"   æ€»æŠ“å–æˆåŠŸç‡: {grasp_success_rate:.1%} ({total_successful_grasps}/{total_grasps})")
        print(f"   æ•°æ®ä¿å­˜ä½ç½®:")
        print(f"     è®­ç»ƒé›†: {self.train_dir} ({self.train_split:.1%})")
        print(f"     æµ‹è¯•é›†: {self.test_dir} ({1-self.train_split:.1%})")
        print("âœ… æ•°æ®æ”¶é›†æˆåŠŸ!")
        
        return successful_scenes > 0

def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–å¯ä¾›æ€§è®­ç»ƒæ•°æ®æ”¶é›†å™¨')
    parser.add_argument('--num_scenes', type=int, default=10, help='æ”¶é›†çš„åœºæ™¯æ•°é‡')
    parser.add_argument('--num_objects', type=int, nargs=2, default=[2, 4], help='ç‰©ä½“æ•°é‡èŒƒå›´ [min, max]')
    parser.add_argument('--max_attempts', type=int, default=25, help='æ¯åœºæ™¯æœ€å¤§æŠ“å–å°è¯•æ•°')
    parser.add_argument('--data_dir', type=str, default='data/affordance_v5', help='æ•°æ®ä¿å­˜ç›®å½•')
    parser.add_argument('--angles', type=int, default=8, help='ç¦»æ•£æŠ“å–è§’åº¦æ•°é‡')
    parser.add_argument('--train_split', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹ (0.0-1.0)')
    parser.add_argument('--gui', action='store_true', help='æ˜¾ç¤ºGUI')
    
    args = parser.parse_args()
    
    # è¿æ¥PyBullet
    if args.gui:
        p.connect(p.GUI)
    else:
        p.connect(p.DIRECT)
    
    try:
        # åˆ›å»ºæ”¶é›†å™¨
        collector = AutoAffordanceCollector(
            data_dir=args.data_dir,
            num_angles=args.angles,
            train_split=args.train_split
        )
        
        # æ”¶é›†æ•°æ®
        success = collector.collect_dataset(
            num_scenes=args.num_scenes,
            num_objects_range=tuple(args.num_objects),
            max_attempts_per_scene=args.max_attempts
        )
        
        if success:
            print("âœ… æ•°æ®æ”¶é›†æˆåŠŸ!")
        else:
            print("âŒ æ•°æ®æ”¶é›†å¤±è´¥!")
    
    finally:
        p.disconnect()

if __name__ == "__main__":
    main()