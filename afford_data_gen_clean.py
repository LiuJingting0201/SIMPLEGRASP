# -*- coding: utf-8 -*-
"""
è‡ªç›‘ç£æŠ“å–å¯ä¾›æ€§æ•°æ®ç”Ÿæˆå™¨ v3 - æ¸…ç†ç‰ˆ
Self-supervised Grasp Affordance Data Generator

ç®€å•    if visualize:
        print(f"   ğŸ“ æ¡Œé¢æ·±åº¦: {table_depth:.3f}m (ç›´æ–¹å›¾å³°å€¼)")
        print(f"   ğŸ“ ç‰©ä½“é˜ˆå€¼: < {depth_threshold:.3f}m (æ¯”æ¡Œé¢è¿‘10mm+)")
        print(f"   ğŸ¯ æ·±åº¦æ£€æµ‹: {depth_based_objects.sum()} åƒç´  ({100*depth_based_objects.sum()/(height*width):.1f}%)")1. ç”¨RGBæ ‡å‡†å·®æ‰¾å½©è‰²ç‰©ä½“
2. ä½¿ç”¨è¿™äº›åƒç´ çš„æ·±åº¦å€¼
3. ç”ŸæˆæŠ“å–å€™é€‰å¹¶æµ‹è¯•
"""

import pybullet as p
import numpy as np
import time
import json
import os
from pathlib import Path
import cv2
import argparse

from environment_setup import setup_environment
from perception import set_topdown_camera, get_rgb_depth, get_rgb_depth_segmentation, pixel_to_world, CAMERA_PARAMS


# ==================== é…ç½®å‚æ•° ====================

DATA_DIR = Path(__file__).parent.parent / "data" / "affordance_v4"
NUM_ANGLES = 16
ANGLE_BINS = np.linspace(0, np.pi, NUM_ANGLES, endpoint=False)

# é‡‡æ ·å‚æ•°
FOREGROUND_STRIDE = 8
BACKGROUND_STRIDE = 64
MIN_DEPTH = 0.01
COLOR_DIFF_THRESHOLD = 30  # é¢œè‰²å·®å¼‚é˜ˆå€¼ï¼šä¸æ¡Œå­é¢œè‰²çš„è·ç¦»ï¼ˆè¶Šå¤§è¶Šä¸¥æ ¼ï¼‰
EDGE_MARGIN = 20  # ä»å›¾åƒè¾¹ç¼˜é‡‡æ ·æ¡Œå­é¢œè‰²çš„è¾¹è·ï¼ˆåƒç´ ï¼‰

# æŠ“å–å‚æ•°
TABLE_TOP_Z = 0.625
PRE_GRASP_OFFSET = 0.25
GRASP_OFFSET = 0.05
POST_GRASP_OFFSET = 0.00
LIFT_HEIGHT = 0.30
GRIPPER_CLOSED = 0.00
FAST_STEPS = 120
SLOW_STEPS = 600


def create_data_dirs():
    """åˆ›å»ºæ•°æ®ç›®å½•"""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {DATA_DIR.absolute()}")


def sample_grasp_candidates(depth, num_angles=NUM_ANGLES, visualize=False, rgb=None, view_matrix=None, proj_matrix=None, seg_mask=None, object_ids=None):
    """åŸºäºPyBullet segmentation maskçš„ç‰©ä½“åˆ†å‰²ç­–ç•¥
    
    ä½¿ç”¨ç›¸æœºçš„å†…ç½®åˆ†å‰²åŠŸèƒ½ç›´æ¥è¯†åˆ«ç‰©ä½“åƒç´ 
    
    Args:
        depth: æ·±åº¦å›¾
        num_angles: è§’åº¦æ•°é‡  
        visualize: æ˜¯å¦å¯è§†åŒ–
        rgb: RGBå›¾åƒ
        view_matrix, proj_matrix: ç›¸æœºçŸ©é˜µï¼ˆç”¨äºè°ƒè¯•åæ ‡è½¬æ¢ï¼‰
        seg_mask: PyBullet segmentation mask
        object_ids: ç‰©ä½“IDåˆ—è¡¨
    """
    height, width = depth.shape
    candidates = []
    
    if seg_mask is None or object_ids is None:
        print(f"   âš ï¸  éœ€è¦segmentation maskå’Œobject IDs")
        return candidates
    
    # Step 1: ä½¿ç”¨PyBullet segmentation maskç›´æ¥è·å–ç‰©ä½“åƒç´   
    object_mask = np.zeros((height, width), dtype=bool)
    for obj_id in object_ids:
        object_mask |= (seg_mask == obj_id)
    
    # è¿‡æ»¤æ‰æ·±åº¦æ— æ•ˆçš„åƒç´ 
    object_mask &= (depth > MIN_DEPTH)
    
    if visualize:
        print(f"   ï¿½ Segmentationæ£€æµ‹: {object_mask.sum()} åƒç´  ({100*object_mask.sum()/(height*width):.1f}%)")
        print(f"   ï¿½ ç‰©ä½“IDs: {object_ids}")
        
        # æ˜¾ç¤ºæ¯ä¸ªç‰©ä½“çš„åƒç´ æ•°
        for obj_id in object_ids:
            obj_pixels = (seg_mask == obj_id).sum()
            print(f"      ç‰©ä½“ ID={obj_id}: {obj_pixels} åƒç´ ")
        
        # æ˜¾ç¤ºç‰©ä½“æ·±åº¦ç»Ÿè®¡
        if object_mask.sum() > 0:
            obj_depths = depth[object_mask]
            valid_obj_depths = obj_depths[obj_depths > MIN_DEPTH]
            if len(valid_obj_depths) > 0:
                print(f"   ğŸ“Š ç‰©ä½“æ·±åº¦: min={valid_obj_depths.min():.3f}m, max={valid_obj_depths.max():.3f}m, mean={valid_obj_depths.mean():.3f}m")
        
        # ä¿å­˜å¯è§†åŒ–
        vis = np.zeros((height, width, 3), dtype=np.uint8)
        
        # ä¸ºæ¯ä¸ªç‰©ä½“åˆ†é…ä¸åŒçš„é¢œè‰²
        colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255), (0, 255, 255)]
        for idx, obj_id in enumerate(object_ids):
            obj_pixels = (seg_mask == obj_id)
            vis[obj_pixels] = colors[idx % len(colors)]
        
        # æ ‡è®°å›¾åƒä¸­å¿ƒ
        center_v, center_u = height // 2, width // 2
        cv2.circle(vis, (center_u, center_v), 5, (255, 255, 255), -1)  # ç™½è‰²=å›¾åƒä¸­å¿ƒ
        
        cv2.imwrite("/tmp/object_detection.png", vis)
        
        # ä¿å­˜æ·±åº¦çƒ­å›¾
        valid_depth = depth[depth > MIN_DEPTH]
        if len(valid_depth) > 0:
            depth_min, depth_max = valid_depth.min(), valid_depth.max()
            depth_vis = np.clip((depth - depth_min) / (depth_max - depth_min + 1e-6) * 255, 0, 255).astype(np.uint8)
            depth_color = cv2.applyColorMap(depth_vis, cv2.COLORMAP_JET)
            depth_color[depth <= MIN_DEPTH] = [0, 0, 0]
            cv2.imwrite("/tmp/depth_heatmap.png", depth_color)
        
        # ä¿å­˜segmentation maskå¯è§†åŒ–
        seg_vis = ((seg_mask % 10) * 25).astype(np.uint8)
        cv2.imwrite("/tmp/segmentation_mask.png", seg_vis)
        
        # ä¿å­˜RGBå›¾åƒ
        if rgb is not None:
            cv2.imwrite("/tmp/rgb_image.png", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))
        
        print(f"   ğŸ’¾ å¯è§†åŒ–å·²ä¿å­˜:")
        print(f"      /tmp/object_detection.png (ç»¿è‰²=æ£€æµ‹åˆ°çš„ç‰©ä½“, é»„è‰²=æ·±åº¦å€™é€‰)")
        print(f"      /tmp/depth_heatmap.png (æ·±åº¦çƒ­å›¾, è“è‰²=è¿‘, çº¢è‰²=è¿œ)")
        print(f"      /tmp/rgb_image.png (åŸå§‹RGBå›¾)")
        
        # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„ç‰©ä½“åŒºåŸŸçš„ä½ç½®
        if object_mask.sum() > 0:
            obj_coords = np.where(object_mask)
            obj_center_v = int(np.mean(obj_coords[0]))
            obj_center_u = int(np.mean(obj_coords[1]))
            print(f"   ğŸ“ æ£€æµ‹åˆ°çš„ç‰©ä½“ä¸­å¿ƒåƒç´ : ({obj_center_u}, {obj_center_v})")
            print(f"   ğŸ“ å›¾åƒä¸­å¿ƒåƒç´ : ({center_u}, {center_v})")
            
            # æ˜¾ç¤ºç‰©ä½“åŒºåŸŸçš„æ·±åº¦æ ·æœ¬
            sample_v, sample_u = obj_coords[0][0], obj_coords[1][0]
            sample_depth = depth[sample_v, sample_u]
            sample_rgb = rgb[sample_v, sample_u]
            print(f"   ğŸ” ç‰©ä½“åŒºåŸŸæ ·æœ¬åƒç´  ({sample_u}, {sample_v}):")
            print(f"      æ·±åº¦={sample_depth:.3f}m, RGB={sample_rgb}")
            
            # æµ‹è¯•åƒç´ åˆ°ä¸–ç•Œåæ ‡çš„è½¬æ¢
            if view_matrix is not None and proj_matrix is not None:
                test_world = pixel_to_world(obj_center_u, obj_center_v, depth[obj_center_v, obj_center_u], view_matrix, proj_matrix)
                print(f"   ğŸ§­ ç‰©ä½“ä¸­å¿ƒåƒç´  â†’ ä¸–ç•Œåæ ‡: [{test_world[0]:.3f}, {test_world[1]:.3f}, {test_world[2]:.3f}]")
    
    if object_mask.sum() == 0:
        print(f"   âš ï¸  æœªæ£€æµ‹åˆ°ç‰©ä½“")
        return candidates
    
    print(f"   ğŸ¯ æ£€æµ‹åˆ°ç‰©ä½“åƒç´ : {object_mask.sum()}")
    
    # Step 3: åœ¨ç‰©ä½“åŒºåŸŸé‡‡æ ·
    # å¯¹äºå°ç‰©ä½“ï¼ˆ< 100åƒç´ ï¼‰ï¼Œç›´æ¥åœ¨æ‰€æœ‰åƒç´ ä¸Šé‡‡æ ·ï¼Œä¸erode
    if object_mask.sum() < 100:
        print(f"   ğŸ¯ å°ç‰©ä½“ï¼šç›´æ¥é‡‡æ ·æ‰€æœ‰åƒç´ ")
        # è·å–æ‰€æœ‰ç‰©ä½“åƒç´ 
        obj_coords = np.where(object_mask)
        obj_center_v = int(np.mean(obj_coords[0]))
        obj_center_u = int(np.mean(obj_coords[1]))
        
        # å¯¹æ¯ä¸ªç‰©ä½“åƒç´ ï¼ŒæŒ‰è·ç¦»ä¸­å¿ƒæ’åº
        positions = []
        for i in range(len(obj_coords[0])):
            v, u = obj_coords[0][i], obj_coords[1][i]
            dist = (u - obj_center_u)**2 + (v - obj_center_v)**2
            positions.append((dist, u, v))
        
        positions.sort()
        
        # ä¸ºæ¯ä¸ªåƒç´ æ·»åŠ æ‰€æœ‰è§’åº¦çš„å€™é€‰
        for theta_idx in range(num_angles):
            theta = ANGLE_BINS[theta_idx]
            for dist, u, v in positions:
                candidates.append((u, v, theta_idx, theta))
    else:
        # å¤§ç‰©ä½“ï¼šä½¿ç”¨erosionæ‰¾åˆ°ä¸­å¿ƒåŒºåŸŸ
        kernel = np.ones((5, 5), np.uint8)
        eroded = cv2.erode(object_mask.astype(np.uint8), kernel, iterations=2)
        
        if eroded.sum() > 0:
            print(f"   ğŸ¯ ç‰©ä½“ä¸­å¿ƒ: {eroded.sum()} åƒç´ ")
            # ä¸­å¿ƒåŒºåŸŸ
            coords = np.where(eroded > 0)
            center_v = int(np.mean(coords[0]))
            center_u = int(np.mean(coords[1]))
            
            positions = []
            for v in range(0, height, 4):
                for u in range(0, width, 4):
                    if eroded[v, u]:
                        dist = (u - center_u)**2 + (v - center_v)**2
                        positions.append((dist, u, v))
            
            positions.sort()
            
            # æ·»åŠ å€™é€‰ï¼ˆä¸åŒè§’åº¦ï¼‰
            for theta_idx in range(num_angles):
                theta = ANGLE_BINS[theta_idx]
                for dist, u, v in positions:
                    candidates.append((u, v, theta_idx, theta))
        
        # è¾¹ç¼˜åŒºåŸŸ
        edge = object_mask & (~eroded.astype(bool))
        if edge.sum() > 0:
            edge_pos = []
            for v in range(0, height, FOREGROUND_STRIDE):
                for u in range(0, width, FOREGROUND_STRIDE):
                    if edge[v, u]:
                        edge_pos.append((u, v))
            
            for theta_idx in [0, 4, 8, 12]:
                if theta_idx < num_angles:
                    theta = ANGLE_BINS[theta_idx]
                    for u, v in edge_pos:
                        candidates.append((u, v, theta_idx, theta))
    
    # èƒŒæ™¯é‡‡æ ·ï¼ˆå‡å°‘èƒŒæ™¯æ ·æœ¬æ•°é‡ï¼Œé‡ç‚¹æµ‹è¯•ç‰©ä½“ï¼‰
    bg_count = 0
    bg_stride = BACKGROUND_STRIDE * 2  # æ›´ç¨€ç–çš„èƒŒæ™¯é‡‡æ ·
    for v in range(0, height, bg_stride):
        for u in range(0, width, bg_stride):
            if not object_mask[v, u] and depth[v, u] > MIN_DEPTH:
                candidates.append((u, v, 0, 0.0))
                bg_count += 1
    
    fg_count = len(candidates) - bg_count
    print(f"   ğŸ“ é‡‡æ · {len(candidates)} ä¸ªå€™é€‰ (å‰æ™¯: {fg_count}, èƒŒæ™¯: {bg_count})")
    
    if visualize and len(candidates) > 100:
        # åœ¨å¯è§†åŒ–æ¨¡å¼ä¸‹ï¼Œä¼˜å…ˆæµ‹è¯•å‰æ™¯å€™é€‰
        print(f"   ğŸ§ª å¯è§†åŒ–æ¨¡å¼ï¼šæµ‹è¯•å‰ 100 ä¸ªï¼ˆä¼˜å…ˆå‰æ™¯ï¼‰")
        candidates = candidates[:100]
    
    return candidates


def fast_grasp_test(robot_id, world_pos, grasp_angle, object_ids, visualize=False):
    """å¿«é€ŸæŠ“å–æµ‹è¯•"""
    ee_link = 11
    steps = SLOW_STEPS if visualize else FAST_STEPS
    
    # æ£€æŸ¥Zåæ ‡
    if world_pos[2] < TABLE_TOP_Z - 0.05 or world_pos[2] > TABLE_TOP_Z + 0.30:
        if visualize:
            print(f"         âš ï¸  Zåæ ‡ä¸åˆç† ({world_pos[2]:.3f}m)")
        return False
    
    # æ£€æŸ¥XYå·¥ä½œç©ºé—´
    dist = np.sqrt(world_pos[0]**2 + world_pos[1]**2)
    if dist < 0.35 or dist > 0.80:
        if visualize:
            print(f"         âš ï¸  è¶…å‡ºå·¥ä½œèŒƒå›´ (è·ç¦»={dist:.3f}m)")
        return False
    
    initial_z = {}
    for obj_id in object_ids:
        pos, _ = p.getBasePositionAndOrientation(obj_id)
        initial_z[obj_id] = pos[2]
    
    try:
        ori = p.getQuaternionFromEuler([np.pi, 0, grasp_angle])
        
        # é¢„æŠ“å–
        if visualize:
            print(f"         â†‘ é¢„æŠ“å–...")
        pre_pos = [world_pos[0], world_pos[1], world_pos[2] + PRE_GRASP_OFFSET]
        if not move_fast(robot_id, ee_link, pre_pos, ori, steps):
            return False
        
        # ä¸‹é™
        if visualize:
            print(f"         â†“ ä¸‹é™...")
        grasp_pos = [world_pos[0], world_pos[1], world_pos[2] + GRASP_OFFSET]
        if not move_fast(robot_id, ee_link, grasp_pos, ori, steps, slow=True):
            return False
        
        # é—­åˆå¤¹çˆª
        if visualize:
            print(f"         ğŸ¤ é—­åˆ...")
        close_gripper_slow(robot_id, steps//2)
        
        # æ£€æŸ¥å¤¹çˆª
        finger_state = p.getJointState(robot_id, 9)[0]
        if finger_state < 0.001:
            if visualize:
                print(f"         âš ï¸  å¤¹çˆªæœªé—­åˆ")
            return False
        
        # æŠ¬èµ·
        if visualize:
            print(f"         â†‘â†‘ æŠ¬èµ·...")
        lift_pos = [grasp_pos[0], grasp_pos[1], world_pos[2] + LIFT_HEIGHT]
        if not move_fast(robot_id, ee_link, lift_pos, ori, steps):
            return False
        
        # åˆ¤æ–­æˆåŠŸ
        for obj_id in object_ids:
            pos, _ = p.getBasePositionAndOrientation(obj_id)
            if pos[2] - initial_z[obj_id] > 0.08:
                if visualize:
                    print(f"         âœ… æˆåŠŸï¼æŠ¬èµ· {(pos[2]-initial_z[obj_id])*100:.1f}cm")
                return True
        
        if visualize:
            print(f"         âŒ å¤±è´¥")
        return False
    
    except Exception as e:
        if visualize:
            print(f"         âš ï¸ å¼‚å¸¸: {e}")
        return False


def move_fast(robot_id, ee_link, target_pos, target_ori, max_steps, slow=False):
    """ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®"""
    ll, ul, jr, rp = [], [], [], []
    for i in range(7):
        info = p.getJointInfo(robot_id, i)
        ll.append(info[8])
        ul.append(info[9])
        jr.append(info[9] - info[8])
        rp.append(p.getJointState(robot_id, i)[0])
    
    joints = p.calculateInverseKinematics(
        robot_id, ee_link, target_pos, target_ori,
        lowerLimits=ll, upperLimits=ul, jointRanges=jr, restPoses=rp
    )
    
    if not joints or len(joints) < 7:
        return False
    
    velocity = 0.3 if slow else 1.0
    force = 300 if slow else 500
    
    for i in range(7):
        p.setJointMotorControl2(
            robot_id, i, p.POSITION_CONTROL,
            targetPosition=joints[i], force=force, maxVelocity=velocity
        )
    
    for _ in range(max_steps):
        p.stepSimulation()
        time.sleep(1./240.)
    
    current = p.getLinkState(robot_id, ee_link)[0]
    dist = np.linalg.norm(np.array(current) - np.array(target_pos))
    return dist < 0.10


def close_gripper_slow(robot_id, steps):
    """æ…¢é€Ÿé—­åˆå¤¹çˆª"""
    pos = GRIPPER_CLOSED / 2.0
    p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, targetPosition=pos, force=50, maxVelocity=0.05)
    p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, targetPosition=pos, force=50, maxVelocity=0.05)
    for _ in range(steps):
        p.stepSimulation()
        time.sleep(1./240.)


def open_gripper_fast(robot_id):
    """æ‰“å¼€å¤¹çˆª"""
    pos = 0.04 / 2.0
    p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, targetPosition=pos, force=50)
    p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, targetPosition=pos, force=50)
    for _ in range(10):
        p.stepSimulation()


def reset_robot_home(robot_id):
    """é‡ç½®æœºå™¨äºº"""
    home = [0, -0.785, 0, -2.356, 0, 1.571, 0.785]
    for i in range(7):
        p.resetJointState(robot_id, i, home[i])
    open_gripper_fast(robot_id)


def generate_scene_data(scene_id, num_objects=3, visualize=False):
    """ç”Ÿæˆå•ä¸ªåœºæ™¯æ•°æ®"""
    print(f"\nğŸ¬ åœºæ™¯ {scene_id:04d}")
    
    client = p.connect(p.GUI if visualize else p.DIRECT)
    if client < 0:
        return False
    
    try:
        robot_id, object_ids = setup_environment(num_objects=num_objects)
        if not object_ids:
            return False
        
        print("   â³ ç­‰å¾…ç‰©ä½“ç¨³å®š...")
        for _ in range(120):
            p.stepSimulation()
        
        if visualize:
            print("   â¸ï¸  æŒ‰ Enter ç»§ç»­...")
            input()
        
        print("   ğŸ“¸ é‡‡é›†å›¾åƒ")
        width, height, view_matrix, proj_matrix = set_topdown_camera()
        rgb, depth, seg_mask = get_rgb_depth_segmentation(width, height, view_matrix, proj_matrix)
        
        if visualize:
            for i, obj_id in enumerate(object_ids):
                pos, _ = p.getBasePositionAndOrientation(obj_id)
                print(f"   ğŸ“¦ ç‰©ä½“{i+1} (ID={obj_id}): [{pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f}]")
            print(f"   ğŸ“Š æ·±åº¦: min={depth.min():.3f}, max={depth.max():.3f}")
            print(f"   ğŸ­ Segmentation mask: {len(np.unique(seg_mask))} ä¸ªä¸åŒID")
        
        candidates = sample_grasp_candidates(depth, NUM_ANGLES, visualize, rgb, view_matrix, proj_matrix, seg_mask, object_ids)
        
        label = np.zeros((height, width, NUM_ANGLES + 1), dtype=np.uint8)
        
        print(f"   ğŸ§ª æµ‹è¯• {len(candidates)} ä¸ªå€™é€‰")
        success_count = 0
        
        for idx, (u, v, theta_idx, theta) in enumerate(candidates):
            if depth[v, u] < MIN_DEPTH:
                continue
            
            if visualize:
                print(f"\n      === å€™é€‰ {idx+1}/{len(candidates)} ===")
                print(f"         åƒç´ : ({u}, {v}), è§’åº¦: {np.degrees(theta):.1f}Â°")
            
            world_pos = pixel_to_world(u, v, depth[v, u], view_matrix, proj_matrix)
            
            if visualize:
                print(f"         ä¸–ç•Œåæ ‡: [{world_pos[0]:.3f}, {world_pos[1]:.3f}, {world_pos[2]:.3f}]")
            
            success = fast_grasp_test(robot_id, world_pos, theta, object_ids, visualize)
            
            if success:
                label[v, u, theta_idx] = 1
                success_count += 1
            
            reset_robot_home(robot_id)
            
            if not visualize and (idx + 1) % 10 == 0:
                print(f"      {idx+1}/{len(candidates)} | æˆåŠŸ: {success_count}")
        
        has_success = label[:, :, :-1].sum(axis=2) > 0
        label[:, :, -1] = (~has_success).astype(np.uint8)
        
        rate = success_count / len(candidates) if len(candidates) > 0 else 0
        print(f"   âœ… æˆåŠŸç‡: {rate*100:.1f}%")
        
        save_scene_data(scene_id, rgb, depth, label, {
            "num_objects": num_objects,
            "num_samples": len(candidates),
            "success_count": success_count,
            "success_rate": rate
        })
        
        return True
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        p.disconnect()


def save_scene_data(scene_id, rgb, depth, label, metadata):
    """ä¿å­˜æ•°æ®"""
    prefix = DATA_DIR / f"scene_{scene_id:04d}"
    cv2.imwrite(str(prefix) + "_rgb.png", cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR))
    np.save(str(prefix) + "_depth.npy", depth)
    np.save(str(prefix) + "_label.npy", label)
    with open(str(prefix) + "_meta.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"   ğŸ’¾ ä¿å­˜: {prefix}_*")


def generate_dataset(num_scenes, num_objects_range=(1, 3), visualize_first=False):
    """æ‰¹é‡ç”Ÿæˆ"""
    print("=" * 60)
    print("ğŸš€ ç”Ÿæˆæ•°æ®é›†")
    print("=" * 60)
    
    create_data_dirs()
    success_scenes = 0
    start = time.time()
    
    for scene_id in range(num_scenes):
        num_objects = np.random.randint(num_objects_range[0], num_objects_range[1] + 1)
        vis = visualize_first and (scene_id == 0)
        
        if generate_scene_data(scene_id, num_objects, vis):
            success_scenes += 1
    
    elapsed = time.time() - start
    print("\n" + "=" * 60)
    print(f"âœ… å®Œæˆï¼{success_scenes}/{num_scenes}")
    print(f"   è€—æ—¶: {elapsed:.1f}s ({elapsed/num_scenes:.1f}s/åœºæ™¯)")
    print(f"   ä½ç½®: {DATA_DIR.absolute()}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_scenes", type=int, default=10)
    parser.add_argument("--num_objects", type=int, nargs=2, default=[1, 3])
    parser.add_argument("--visualize_first", action="store_true")
    args = parser.parse_args()
    
    generate_dataset(
        num_scenes=args.num_scenes,
        num_objects_range=tuple(args.num_objects),
        visualize_first=args.visualize_first
    )


if __name__ == "__main__":
    main()
