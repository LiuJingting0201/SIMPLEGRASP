# -*- coding: utf-8 -*-
"""
è‡ªç›‘ç£æŠ“å–å¯ä¾›æ€§ï¼ˆAffordanceï¼‰æ•°æ®æ”¶é›†å™¨
æŒ‰ç…§æ–¹æ¡ˆAçš„æ€è·¯ï¼šå¯¹æ¯ä¸€å¸§å›¾åƒåœ¨æ¡Œé¢ä¸Šé‡‡æ ·è‹¥å¹²æŠ“å–å€™é€‰ï¼ˆä½ç½®+è§’åº¦ï¼‰ï¼Œ
ç”¨ç‰©ç†ä»¿çœŸ"è¯•æŠ“"æ‰“æ ‡ç­¾ï¼ˆæˆåŠŸ/å¤±è´¥ï¼‰ï¼Œç”Ÿæˆå¯ç›‘ç£æ•°æ®ã€‚
"""

import numpy as np
import cv2
import os
import json
import time
from pathlib import Path
import argp                    results.append(False)
            
            # 5. ç”Ÿæˆå¯ä¾›æ€§åœ°å›¾
            print("   ï¿½ï¸  ç”Ÿæˆå¯ä¾›æ€§çƒ­åŠ›å›¾...")ort pybullet as p

# å¯¼å…¥ç°æœ‰æ¨¡å—
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.environment_setup import setup_environment
from src.perception import set_topdown_camera, get_rgb_depth_segmentation
from src.afford_data_gen import sample_grasp_candidates, fast_grasp_test, reset_robot_home
from src.perception import set_topdown_camera, get_rgb_depth_segmentation
from src.afford_data_gen import sample_grasp_candidates, fast_grasp_test, reset_robot_home

class AffordanceDataCollector:
    """å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, image_width=224, image_height=224, num_angles=8):
        """
        åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨ - ä¸åŸå§‹å·¥ä½œç³»ç»Ÿä¿æŒä¸€è‡´
        Args:
            image_width: ç›¸æœºå›¾åƒå®½åº¦ (é»˜è®¤224ï¼Œä¸åŸå§‹ç³»ç»Ÿä¸€è‡´)
            image_height: ç›¸æœºå›¾åƒé«˜åº¦ (é»˜è®¤224ï¼Œä¸åŸå§‹ç³»ç»Ÿä¸€è‡´) 
            num_angles: ç¦»æ•£åŒ–çš„æŠ“å–è§’åº¦æ•°é‡ï¼ˆæ¯45åº¦ä¸€ä¸ªè§’åº¦ï¼‰
        """
        self.image_width = image_width
        self.image_height = image_height
        self.num_angles = num_angles
        
        # å®šä¹‰ç¦»æ•£åŒ–çš„æŠ“å–è§’åº¦ (0åº¦åˆ°315åº¦ï¼Œæ¯45åº¦ä¸€ä¸ª)
        self.grasp_angles = np.linspace(0, 2*np.pi, num_angles, endpoint=False)
        
        # è®¾ç½®ç›¸æœºå‚æ•° - ä¸åŸå§‹å·¥ä½œç³»ç»Ÿä¿æŒä¸€è‡´
        self.width, self.height = image_width, image_height
        self.fov = 60.0
        self.near = 0.1
        self.far = 2.0
        
        # æ•°æ®ä¿å­˜è·¯å¾„
        self.data_dir = Path("data/affordance_dataset")
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¯ åˆå§‹åŒ–å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨")
        print(f"   å›¾åƒå°ºå¯¸: {image_width}x{image_height}")
        print(f"   æŠ“å–è§’åº¦æ•°é‡: {num_angles} (æ¯{360/num_angles:.0f}åº¦ä¸€ä¸ª)")
        print(f"   æ•°æ®ä¿å­˜è·¯å¾„: {self.data_dir.absolute()}")
    
    def capture_camera_data(self):
        """æ•è·ç›¸æœºæ•°æ®"""
        # è®¾ç½®ä¿¯è§†ç›¸æœº
        width, height, view_matrix, proj_matrix = set_topdown_camera(
            target=[0.60, 0, 0.625],  # å¯¹å‡†ç‰©ä½“ç”ŸæˆåŒºåŸŸ
            distance=1.2,
            yaw=90.0,
            pitch=-89.0,
            width=self.image_width,
            height=self.image_height
        )
        
        # è·å–RGBã€æ·±åº¦å’Œåˆ†å‰²å›¾åƒ
        rgb_image, depth_image, seg_mask = get_rgb_depth_segmentation(
            width, height, view_matrix, proj_matrix
        )
        
        # è®¡ç®—ç›¸æœºå†…å‚çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        fov_rad = np.radians(60.0)  # 60åº¦è§†é‡
        f = height / (2 * np.tan(fov_rad / 2))
        camera_intrinsics = np.array([
            [f, 0, width/2],
            [0, f, height/2],
            [0, 0, 1]
        ])
        
        # åˆ›å»ºç‰©ä½“æ©ç ï¼ˆéèƒŒæ™¯åŒºåŸŸï¼‰
        mask_image = (seg_mask > 2).astype(np.uint8)  # è·³è¿‡å¹³é¢(0)ã€æ¡Œå­(1)ã€æœºå™¨äºº(2)
        
        return rgb_image, depth_image, camera_intrinsics, mask_image, view_matrix, proj_matrix, seg_mask
    
    def pixel_to_world_corrected(self, u, v, depth_value):
        """ä»åƒç´ åæ ‡è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ - ä½¿ç”¨åŸå§‹å·¥ä½œç³»ç»Ÿçš„æ–¹æ³•"""
        # ä½¿ç”¨ä¸perception.pyç›¸åŒçš„ç›¸æœºå‚æ•°
        camera_target = [0.60, 0, 0.625]
        camera_distance = 1.2
        
        # 1. PyBulletæ·±åº¦ç¼“å†²åŒº â†’ å®é™…è·ç¦»ï¼ˆæ²¿ç›¸æœºå…‰è½´ï¼‰
        depth_real = self.far * self.near / (self.far - (self.far - self.near) * depth_value)
        
        # 2. è®¡ç®—ç›¸æœºåœ¨è¯¥æ·±åº¦å¤„çš„è§†é‡èŒƒå›´
        fov_rad = np.radians(self.fov)
        view_width_at_depth = 2.0 * depth_real * np.tan(fov_rad / 2.0)
        view_height_at_depth = view_width_at_depth  # æ­£æ–¹å½¢è§†é‡
        
        # 3. åƒç´ åæ ‡å½’ä¸€åŒ–åˆ° [-0.5, 0.5]
        u_norm = (u / self.width) - 0.5
        v_norm = (v / self.height) - 0.5
        
        # 4. è®¡ç®—XYä¸–ç•Œåæ ‡
        # ç›¸æœºä¿¯è§†ï¼Œyaw=90åº¦ï¼ˆç›¸æœºç»•Zè½´æ—‹è½¬90Â°ï¼‰ï¼š
        # - å›¾åƒå·¦å³(u) å¯¹åº”ä¸–ç•Œ Yè½´ï¼ˆuå°=å·¦=å=-Yï¼Œuå¤§=å³=å‰=+Yï¼‰
        # - å›¾åƒä¸Šä¸‹(v) å¯¹åº”ä¸–ç•Œ Xè½´ï¼ˆvå°=ä¸Š=å·¦=-Xï¼Œvå¤§=ä¸‹=å³=+Xï¼‰
        y_world = camera_target[1] + u_norm * view_width_at_depth   # u â†’ Y
        x_world = camera_target[0] + v_norm * view_height_at_depth  # v â†’ X
        
        # 5. è®¡ç®—Zåæ ‡
        # ç›¸æœºé«˜åº¦ = ç›®æ ‡é«˜åº¦ + ç›¸æœºè·ç¦»
        camera_height = camera_target[2] + camera_distance
        z_world = camera_height - depth_real
        
        return np.array([x_world, y_world, z_world])
    
    def collect_scene_data(self, scene_id, num_objects=3, num_samples=50):
        """
        æ”¶é›†å•ä¸ªåœºæ™¯çš„å¯ä¾›æ€§æ•°æ®
        Args:
            scene_id: åœºæ™¯ID
            num_objects: ç‰©ä½“æ•°é‡
            num_samples: æ¯ä¸ªåœºæ™¯çš„é‡‡æ ·ç‚¹æ•°é‡
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ”¶é›†æ•°æ®
        """
        print(f"\nğŸ”¬ æ”¶é›†åœºæ™¯ {scene_id} çš„å¯ä¾›æ€§æ•°æ®...")
        print(f"   ç‰©ä½“æ•°é‡: {num_objects}")
        print(f"   é‡‡æ ·ç‚¹æ•°é‡: {num_samples}")
        
        try:
            # 1. è®¾ç½®ç¯å¢ƒ
            robot_id, object_ids = setup_environment(num_objects=num_objects)
            
            # ç¡®ä¿æœºå™¨äººåœ¨åˆå§‹ä½ç½®
            print("   ğŸ  é‡ç½®æœºå™¨äººåˆ°åˆå§‹ä½ç½®...")
            reset_robot_home(robot_id)
            
            # ç­‰å¾…æœºå™¨äººç¨³å®š
            for _ in range(60):
                p.stepSimulation()
                time.sleep(1./240.)
            
            # 2. æ‹æ‘„ç›¸æœºæ•°æ®
            rgb_image, depth_image, camera_intrinsics, mask_image, view_matrix, proj_matrix, seg_mask = self.capture_camera_data()
            
            if rgb_image is None:
                print("   âŒ ç›¸æœºæ•°æ®è·å–å¤±è´¥")
                return False
            
            print(f"   ğŸ“· è·å–ç›¸æœºæ•°æ®: RGB {rgb_image.shape}, æ·±åº¦ {depth_image.shape}")
            
            # 3. ä½¿ç”¨åŸå§‹çš„é‡‡æ ·å‡½æ•°
            candidates = sample_grasp_candidates(
                depth=depth_image,
                num_angles=self.num_angles,
                visualize=False,
                rgb=rgb_image,
                view_matrix=view_matrix,
                proj_matrix=proj_matrix,
                seg_mask=seg_mask,
                object_ids=object_ids
            )
            
            if not candidates:
                print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„æŠ“å–å€™é€‰ç‚¹")
                return False
            
            # 4. æµ‹è¯•æ¯ä¸ªå€™é€‰ç‚¹
            print(f"   ğŸ§ª å¼€å§‹æµ‹è¯• {len(candidates)} ä¸ªæŠ“å–å€™é€‰ç‚¹...")
            results = []
            success_count = 0
            
            for i, candidate in enumerate(candidates):
                if i % 10 == 0:
                    print(f"      è¿›åº¦: {i+1}/{len(candidates)}, å½“å‰æˆåŠŸ: {success_count}")
                
                # è§£æå€™é€‰ç‚¹æ ¼å¼ (u, v, theta_idx, theta)
                if len(candidate) == 4:
                    u, v, theta_idx, theta = candidate
                else:
                    u, v, theta_idx = candidate
                    theta = self.grasp_angles[theta_idx]
                
                # åƒç´ åˆ°ä¸–ç•Œåæ ‡è½¬æ¢
                try:
                    world_pos = self.pixel_to_world_corrected(u, v, depth_image[v, u])
                    if world_pos is None or len(world_pos) != 3:
                        results.append(False)
                        continue
                    
                    # ç®€å•çš„å·¥ä½œç©ºé—´æ£€æŸ¥
                    dist = np.sqrt(world_pos[0]**2 + world_pos[1]**2)
                    if dist < 0.3 or dist > 0.85 or abs(world_pos[1]) > 0.5:
                        results.append(False)
                        continue
                        
                except Exception as e:
                    results.append(False)
                    continue
                
                # ä½¿ç”¨åŸå§‹çš„æŠ“å–æµ‹è¯•å‡½æ•°
                try:
                    success = fast_grasp_test(
                        robot_id=robot_id,
                        world_pos=world_pos,
                        grasp_angle=theta,
                        object_ids=object_ids,
                        visualize=False,
                        debug_mode=False
                    )
                    results.append(success)
                    if success:
                        success_count += 1
                        print(f"      âœ… æˆåŠŸ: åƒç´ ({u}, {v}) -> ä¸–ç•Œ[{world_pos[0]:.3f}, {world_pos[1]:.3f}, {world_pos[2]:.3f}], è§’åº¦{np.degrees(theta):.1f}Â°")
                except Exception as e:
                    print(f"      âŒ æŠ“å–æµ‹è¯•å¤±è´¥: {e}")
                    results.append(False)
                
                # é‡ç½®æœºå™¨äººå’Œç‰©ä½“çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if len(results) > 0 and results[-1] or np.random.random() < 0.1:  # æˆåŠŸæŠ“å–åæˆ–10%æ¦‚ç‡é‡ç½®
                    object_ids = reset_objects_after_grasp(object_ids, min_objects=num_objects)
            
            # 5. ç”Ÿæˆå¯ä¾›æ€§åœ°å›¾
            print("   ï¿½ï¸  ç”Ÿæˆå¯ä¾›æ€§çƒ­åŠ›å›¾...")
            
            # è½¬æ¢å€™é€‰ç‚¹æ ¼å¼ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            converted_candidates = []
            for i, candidate in enumerate(candidates):
                if len(candidate) == 4:
                    u, v, theta_idx, theta = candidate
                else:
                    u, v, theta_idx = candidate
                converted_candidates.append((u, v, theta_idx))
            
            affordance_map, angle_map = self.generate_affordance_maps(converted_candidates, results)
            
            # 6. ä¿å­˜æ•°æ®
            self.save_scene_data(
                scene_id, rgb_image, depth_image, affordance_map, 
                angle_map, converted_candidates, results, camera_intrinsics
            )
            
            # ç»Ÿè®¡ä¿¡æ¯
            success_rate = np.mean(results)
            print(f"   âœ… åœºæ™¯ {scene_id} æ•°æ®æ”¶é›†å®Œæˆ")
            print(f"      æ€»ä½“æˆåŠŸç‡: {success_rate:.2%}")
            print(f"      å¯ä¾›æ€§çƒ­åŠ›å›¾éé›¶åƒç´ : {np.sum(affordance_map > 0)}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ åœºæ™¯ {scene_id} æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def generate_affordance_maps(self, candidates, results):
        """
        ä»æµ‹è¯•ç»“æœç”Ÿæˆå¯ä¾›æ€§çƒ­åŠ›å›¾
        Args:
            candidates: [(u, v, angle_idx), ...] å€™é€‰ç‚¹åˆ—è¡¨
            results: [True/False, ...] å¯¹åº”çš„æˆåŠŸ/å¤±è´¥ç»“æœ
        Returns:
            tuple: (affordance_map, angle_map)
                affordance_map: (H, W) æ¯ä¸ªåƒç´ çš„æœ€ä½³æŠ“å–æˆåŠŸæ¦‚ç‡
                angle_map: (H, W) æ¯ä¸ªåƒç´ çš„æœ€ä½³æŠ“å–è§’åº¦ç´¢å¼•
        """
        # åˆå§‹åŒ–åœ°å›¾
        affordance_map = np.zeros((self.image_height, self.image_width), dtype=np.float32)
        angle_map = np.full((self.image_height, self.image_width), -1, dtype=np.int8)
        
        # è®°å½•æ¯ä¸ªåƒç´ ä½ç½®çš„æ‰€æœ‰æµ‹è¯•ç»“æœ
        pixel_results = {}  # (u, v): [(angle_idx, success), ...]
        
        for candidate, success in zip(candidates, results):
            u, v, angle_idx = candidate
            
            # ç¡®ä¿åæ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if 0 <= u < self.image_width and 0 <= v < self.image_height:
                if (u, v) not in pixel_results:
                    pixel_results[(u, v)] = []
                
                pixel_results[(u, v)].append((angle_idx, success))
        
        # ä¸ºæ¯ä¸ªåƒç´ è®¡ç®—æœ€ä½³æŠ“å–æ¦‚ç‡å’Œè§’åº¦
        for (u, v), angle_results in pixel_results.items():
            # è®¡ç®—æ¯ä¸ªè§’åº¦çš„æˆåŠŸç‡
            angle_success_rates = {}
            for angle_idx, success in angle_results:
                if angle_idx not in angle_success_rates:
                    angle_success_rates[angle_idx] = []
                angle_success_rates[angle_idx].append(success)
            
            # æ‰¾åˆ°æˆåŠŸç‡æœ€é«˜çš„è§’åº¦
            best_angle_idx = -1
            best_success_rate = 0.0
            
            for angle_idx, successes in angle_success_rates.items():
                success_rate = np.mean(successes)
                if success_rate > best_success_rate:
                    best_success_rate = success_rate
                    best_angle_idx = angle_idx
            
            # è®°å½•ç»“æœ
            affordance_map[v, u] = best_success_rate
            angle_map[v, u] = best_angle_idx
        
        return affordance_map, angle_map
    
    def save_scene_data(self, scene_id, rgb_image, depth_image, affordance_map, 
                       angle_map, candidates, results, camera_intrinsics):
        """ä¿å­˜åœºæ™¯æ•°æ®"""
        scene_name = f"scene_{scene_id:04d}"
        
        # ä¿å­˜å›¾åƒæ•°æ®
        cv2.imwrite(str(self.data_dir / f"{scene_name}_rgb.png"), 
                   cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
        np.save(str(self.data_dir / f"{scene_name}_depth.npy"), depth_image)
        
        # ä¿å­˜å¯ä¾›æ€§æ•°æ®
        np.save(str(self.data_dir / f"{scene_name}_affordance.npy"), affordance_map)
        np.save(str(self.data_dir / f"{scene_name}_angles.npy"), angle_map)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "scene_id": int(scene_id),
            "image_size": [int(self.image_width), int(self.image_height)],
            "num_angles": int(self.num_angles),
            "grasp_angles_rad": [float(x) for x in self.grasp_angles.tolist()],
            "camera_intrinsics": [[float(x) for x in row] for row in camera_intrinsics.tolist()],
            "num_candidates": int(len(candidates)),
            "success_rate": float(np.mean(results)) if results else 0.0,
            "candidates": [[int(u), int(v), int(angle_idx)] for u, v, angle_idx in candidates],  # ç¡®ä¿æ˜¯åŸºæœ¬ç±»å‹
            "results": [bool(r) for r in results],  # ç¡®ä¿æ˜¯åŸºæœ¬ç±»å‹
        }
        
        with open(self.data_dir / f"{scene_name}_meta.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"   ğŸ’¾ åœºæ™¯æ•°æ®å·²ä¿å­˜åˆ° {self.data_dir}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ”¶é›†è‡ªç›‘ç£æŠ“å–å¯ä¾›æ€§æ•°æ®")
    parser.add_argument("--num_scenes", type=int, default=100, 
                       help="è¦æ”¶é›†çš„åœºæ™¯æ•°é‡")
    parser.add_argument("--num_objects", type=int, nargs=2, default=[3, 5],
                       help="æ¯ä¸ªåœºæ™¯çš„ç‰©ä½“æ•°é‡èŒƒå›´")
    parser.add_argument("--num_samples", type=int, default=50,
                       help="æ¯ä¸ªåœºæ™¯çš„æŠ“å–å€™é€‰ç‚¹æ•°é‡")
    parser.add_argument("--num_angles", type=int, default=8,
                       help="ç¦»æ•£åŒ–æŠ“å–è§’åº¦æ•°é‡")
    parser.add_argument("--visualize", action="store_true",
                       help="å¯ç”¨å¯è§†åŒ–ï¼ˆä»…ç¬¬ä¸€ä¸ªåœºæ™¯ï¼‰")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¯ è‡ªç›‘ç£æŠ“å–å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨")
    print("=" * 60)
    print(f"åœºæ™¯æ•°é‡: {args.num_scenes}")
    print(f"ç‰©ä½“æ•°é‡èŒƒå›´: {args.num_objects[0]}-{args.num_objects[1]}")
    print(f"æ¯åœºæ™¯é‡‡æ ·æ•°: {args.num_samples}")
    print(f"æŠ“å–è§’åº¦æ•°: {args.num_angles}")
    print("=" * 60)
    
    # åˆå§‹åŒ–PyBullet
    if args.visualize:
        p.connect(p.GUI)
    else:
        p.connect(p.DIRECT)
    
    # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    collector = AffordanceDataCollector(num_angles=args.num_angles)
    
    try:
        # æ”¶é›†æ•°æ®
        successful_scenes = 0
        
        for scene_id in range(args.num_scenes):
            # éšæœºé€‰æ‹©ç‰©ä½“æ•°é‡
            num_objects = np.random.randint(args.num_objects[0], args.num_objects[1] + 1)
            
            # æ”¶é›†åœºæ™¯æ•°æ®
            if collector.collect_scene_data(scene_id, num_objects, args.num_samples):
                successful_scenes += 1
            
            print(f"è¿›åº¦: {scene_id + 1}/{args.num_scenes} åœºæ™¯, "
                  f"æˆåŠŸ: {successful_scenes}")
        
        print("\n" + "=" * 60)
        print(f"âœ… æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"æˆåŠŸæ”¶é›†åœºæ™¯: {successful_scenes}/{args.num_scenes}")
        print(f"æ•°æ®ä¿å­˜è·¯å¾„: {collector.data_dir.absolute()}")
        print("=" * 60)
        
    finally:
        p.disconnect()

if __name__ == "__main__":
    main()